{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sojaegeon/2023_hands_C/blob/main/N_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghdvmIsv2jul"
      },
      "source": [
        "### 1. 실습명 : N-gram 언어 모델로 문장 생성하기\n",
        "### 2. 실습 목적 및 설명\n",
        "\n",
        "*   파이썬의 NLTK 패키지를 이용하여 N-gram 언어 모델을 구축한다\n",
        "*   네이버에서 오픈 소스로 제공하는 nsmc 영화 리뷰 데이터셋을 이용해 문장을 생성한다.\n",
        "\n",
        "### 4. 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex4VLpId3e-T"
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk import word_tokenize\n",
        "from nltk import ConditionalFreqDist\n",
        "from nltk.probability import ConditionalProbDist, MLEProbDist\n",
        "import numpy as np\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wazYRi07LIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc923fe3-1a07-4183-9469-491f5f97b519"
      },
      "source": [
        "# 한국어 처리에 필요한 konlpy 패키지를 설치하기 전에 선행 파일을 설치한다.\n",
        "!apt-get update\n",
        "\n",
        "!apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "\n",
        "!pip3 install JPype1-py3\n",
        "\n",
        "!pip3 install konlpy\n",
        "\n",
        "!JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,684 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,784 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,737 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,892 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,239 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,538 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,049 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,045 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 29.9 MB in 4s (6,934 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Package python-dev is not available, but is referred to by another package.\n",
            "This may mean that the package is missing, has been obsoleted, or\n",
            "is only available from another source\n",
            "However the following packages replace it:\n",
            "  python2-dev python2 python-dev-is-python3\n",
            "\n",
            "E: Package 'python-dev' has no installation candidate\n",
            "Collecting JPype1-py3\n",
            "  Downloading JPype1-py3-0.5.5.4.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: JPype1-py3\n",
            "  Building wheel for JPype1-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for JPype1-py3: filename=JPype1_py3-0.5.5.4-cp311-cp311-linux_x86_64.whl size=3259382 sha256=37631fd32c5d19057d9ce7a3d79aa20596b0db0cfed6266dd6a1f9c930d7e4ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/a6/b5/d0acc5a6e1622b48518a0ac7266a98778336a0621b532e8f06\n",
            "Successfully built JPype1-py3\n",
            "Installing collected packages: JPype1-py3\n",
            "Successfully installed JPype1-py3-0.5.5.4\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.3.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (24.2)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.5.2 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJbbAta148Tc"
      },
      "source": [
        "# NLTK 패키지를 이용하여 입력 텍스트를 N-gram 형태로 변환한다.\n",
        "sentence = \"나는 매일 아침 지하철을 탄다\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvvfewcs5OR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ae6e58-466a-4835-8724-b8973b5100bb"
      },
      "source": [
        "# NLTK 사용을 위하여 선행 패키지를 설치한다.\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# 입력 텍스트를 띄어쓰기 기준으로 토큰화한다.\n",
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나는', '매일', '아침', '지하철을', '탄다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# '나는 매일 지하철을 탄다'를 형태소 태깅(POS tagging)\n",
        "tagger.pos(sentence)"
      ],
      "metadata": {
        "id": "_8DncjCp2Ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "e072eaf1-050d-46f6-d023-cd59dcc141a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tagger' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3ab9eb29e59d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# '나는 매일 지하철을 탄다'를 형태소 태깅(POS tagging)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tagger' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLTfZw_fcxhs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81beedc6-a85c-42b7-c755-7b932a736971"
      },
      "source": [
        "# 한국어의 단어는 띄어쓰기를 기준으로 하지 않기 때문에 konlpy를 이용해 형태소를 기준으로 토큰화한다.\n",
        "tagger = Okt()\n",
        "\n",
        "def tokenize(text):\n",
        "  tokens = ['/'.join(t) for t in tagger.pos(text)]\n",
        "  # for문의 첫번째 loop: t = ('나', Noun'), '/'.join(t) = '나/Noun'\n",
        "  return tokens\n",
        "\n",
        "tokens = tokenize(sentence)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['나/Noun', '는/Josa', '매일/Noun', '아침/Noun', '지하철/Noun', '을/Josa', '탄다/Verb']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgWgUE_N5yEf"
      },
      "source": [
        "# 토큰을 N-gram의 형태로 바꾸어준다.\n",
        "# ngrams 함수의 두 번째 인자로 N값을 지정할 수 있다.\n",
        "bigram = ngrams(tokens, 2)\n",
        "trigram = ngrams(tokens, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FTnRG9O63WX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d02c59a6-ad4b-4a51-9d5b-59946c87389f"
      },
      "source": [
        "# N-gram을 출력해본다.\n",
        "print(\"bigram: \")\n",
        "for b in bigram:\n",
        "  print(b)\n",
        "\n",
        "print(\"\\ntrigram: \")\n",
        "for t in trigram:\n",
        "  print(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram: \n",
            "('나/Noun', '는/Josa')\n",
            "('는/Josa', '매일/Noun')\n",
            "('매일/Noun', '아침/Noun')\n",
            "('아침/Noun', '지하철/Noun')\n",
            "('지하철/Noun', '을/Josa')\n",
            "('을/Josa', '탄다/Verb')\n",
            "\n",
            "trigram: \n",
            "('나/Noun', '는/Josa', '매일/Noun')\n",
            "('는/Josa', '매일/Noun', '아침/Noun')\n",
            "('매일/Noun', '아침/Noun', '지하철/Noun')\n",
            "('아침/Noun', '지하철/Noun', '을/Josa')\n",
            "('지하철/Noun', '을/Josa', '탄다/Verb')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0MpXv1H6_Gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52af08a4-5b04-4d6b-ba30-7caeda6db90e"
      },
      "source": [
        "# padding을 통해 입력 데이터에 문장의 시작과 끝을 알리는 토큰을 추가한다.\n",
        "# </s> 사용하는 이유: 다음에 나올 확률이 제일 높은 토큰이 </s>이면 글쓰기를 멈추고 문장을 완결지을 수 있음(GPT에서도 동일)\n",
        "bigram = ngrams(tokens, 2, pad_left=True, pad_right=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\")\n",
        "print(\"bigrams with padding: \")\n",
        "for b in bigram:\n",
        "  print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams with padding: \n",
            "('<s>', '나/Noun')\n",
            "('나/Noun', '는/Josa')\n",
            "('는/Josa', '매일/Noun')\n",
            "('매일/Noun', '아침/Noun')\n",
            "('아침/Noun', '지하철/Noun')\n",
            "('지하철/Noun', '을/Josa')\n",
            "('을/Josa', '탄다/Verb')\n",
            "('탄다/Verb', '</s>')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr25hyHn9Shh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb47898-c665-4185-fc74-42cf29475cfc"
      },
      "source": [
        "# 문장 생성을 위하여 네이버 영화 리뷰 데이터셋을 다운로드한다.\n",
        "%%time\n",
        "!wget -nc -q https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.99 ms, sys: 85 µs, total: 6.07 ms\n",
            "Wall time: 205 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS64JsKs92Xi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e780c1f-5a61-46fb-9c5f-5ac8b47559bf"
      },
      "source": [
        "# 다운로드 받은 데이터셋을 읽고 인덱스와 라벨을 제외한 텍스트 부분만 가져온다.\n",
        "# codecs 패키지는 대용량 파일을 조금씩 읽을 수 있게 해준다.\n",
        "\n",
        "with codecs.open(\"ratings_train.txt\", encoding='utf-8') as f: # f라는 변수에 \"ratings_train.txt\" 파일을 불러옴\n",
        "  data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "  # f.read().splitlines : 불러온 파일을 행 단위로 분리해 리스트에 저장\n",
        "  # for line in f.read().splitlines : 행을 담고 있는 리스트에서 행을 1개씩 변수 line에 가져옴 -> 모두 가져올 때까지 반복\n",
        "  # line.split('\\t') : 가져온 행을 '\\t'라는 문자를 기준으로 분리해 리스트로 저장 ex) 'C\\tPython\\tJava'->['C', 'Python', 'Java']\n",
        "  data = data[1:] # data의 0번째 인덱스를 제외함\n",
        "\n",
        "# 데이터셋 형태 : 행과 열로 구분되어 있고, 한 행에서 서로 다른 열의 데이터는 \\t(tab)으로 구분됨. 열 이름은 첫 행에 있음(헤더)\n",
        "\n",
        "print(\"데이터셋: \", data[:10])\n",
        "docs = [row[1] for row in data] # 텍스트 부분만 가져옴\n",
        "print(\"\\n텍스트 데이터:\", docs[:5])\n",
        "print(\"\\n문장 개수: \",len(docs)) # 총 15만개의 문장으로 이루어진 데이터셋임을 알 수 있다."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터셋:  [['9976970', '아 더빙.. 진짜 짜증나네요 목소리', '0'], ['3819312', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '1'], ['10265843', '너무재밓었다그래서보는것을추천한다', '0'], ['9045019', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '0'], ['6483659', '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다', '1'], ['5403919', '막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.', '0'], ['7797314', '원작의 긴장감을 제대로 살려내지못했다.', '0'], ['9443947', '별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네', '0'], ['7156791', '액션이 없는데도 재미 있는 몇안되는 영화', '1'], ['5912145', '왜케 평점이 낮은건데? 꽤 볼만한데.. 헐리우드식 화려함에만 너무 길들여져 있나?', '1']]\n",
            "\n",
            "텍스트 데이터: ['아 더빙.. 진짜 짜증나네요 목소리', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '너무재밓었다그래서보는것을추천한다', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다']\n",
            "\n",
            "문장 개수:  150000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAd9C--T9_AP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99bab936-0768-4c62-fc59-06c151fe3c7e"
      },
      "source": [
        "# 토큰화한 텍스트 데이터의 bigram을 모두 리스트에 추가한다.\n",
        "sentences = []\n",
        "for d in tqdm(docs):\n",
        "  tokens = tokenize(d)\n",
        "  bigram = ngrams(tokens, 2, pad_left=True, pad_right=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\")\n",
        "  sentences += [t for t in bigram] # 리스트로 변환한 뒤 sentences에 추가"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150000/150000 [10:26<00:00, 239.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Pf91qqj9PH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ae28d2-adab-4613-e664-172af121cd7a"
      },
      "source": [
        "print(sentences[:10])\n",
        "print('\\nbi-gram 개수: ', len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<s>', '아/Exclamation'), ('아/Exclamation', '더빙/Noun'), ('더빙/Noun', '../Punctuation'), ('../Punctuation', '진짜/Noun'), ('진짜/Noun', '짜증나네요/Adjective'), ('짜증나네요/Adjective', '목소리/Noun'), ('목소리/Noun', '</s>'), ('<s>', '흠/Noun'), ('흠/Noun', '.../Punctuation'), ('.../Punctuation', '포스터/Noun')]\n",
            "\n",
            "bi-gram 개수:  2306163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL31DVi1ld25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a828d64f-3845-4133-a14d-81eff61b1b64"
      },
      "source": [
        "cfd = ConditionalFreqDist(sentences) # 조건부 빈도 분포: 특정 단어 다음에 어떤 단어가 몇 번 나왔는지 기록\n",
        "print(cfd[\"<s>\"].most_common(5)) # <s> : 시작 토큰, <s> 다음에 가장 많이 나온 단어는 곧 문장을 시작할 때 가장 많이 나오는 단어"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('정말/Noun', 2718), ('이/Noun', 2371), ('진짜/Noun', 2232), ('이/Determiner', 2115), ('영화/Noun', 2069)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYm2U1nhndI6"
      },
      "source": [
        "# 주어진 토큰(c) 다음에 가장 많이 등장하는 n개의 단어를 반환하는 함수를 만든다.\n",
        "def most_common(c, n, pos=None):\n",
        "  if pos is None:\n",
        "    return cfd[tokenize(c)[0]].most_common(n)\n",
        "  else:\n",
        "    return cfd[\"/\".join([c, pos])].most_common(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73MTcDKPoliE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32579bdf-4fd3-422f-aabe-05f4eeda4fae"
      },
      "source": [
        "print(most_common(\"나\", 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('는/Josa', 831), ('의/Josa', 339), ('만/Josa', 213), ('에게/Josa', 148), ('에겐/Josa', 84), ('랑/Josa', 81), ('한테/Josa', 50), ('참/Verb', 45), ('이/Determiner', 44), ('와도/Josa', 43)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaoisqGFnZaK"
      },
      "source": [
        "# 단어별 등장 빈도를 기반으로 조건부 확률을 추정 : 특정 단어 다음에 어떤 단어가 나올 확률\n",
        "cpd = ConditionalProbDist(cfd, MLEProbDist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrIUwvsOmv17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1925ab43-0fb0-4564-d166-0e0fd55dbe57"
      },
      "source": [
        "# “.” 다음에 “</s>”가 올 확률을 출력한다.\n",
        "print(cpd[tokenize(\".\")[0]].prob(\"</s>\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.39102658679807606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPJO9Bw9sLcL"
      },
      "source": [
        "# 토큰 c 다음에 토큰 w가 bigram으로 함께 등장할 확률을 구한다.\n",
        "def bigram_prob(c, w):\n",
        "  context = tokenize(c)[0]\n",
        "  word = tokenize(w)[0]\n",
        "  return cpd[context].prob(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W56LrNYs6MO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcddc002-d88d-4ba3-d08b-c4ee4a80d7fa"
      },
      "source": [
        "print(bigram_prob(\"이\", \"영화\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4010748656417948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80QHPkHXs9qX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f94fcba3-5c3c-4a4d-dfb7-5fbb60058990"
      },
      "source": [
        "print(bigram_prob(\"영화\", \"이\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00015767585785521414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jat3RD0LtJ8Z"
      },
      "source": [
        "# 조건부 확률을 알게 되면 가장 확률이 높은 토큰열을 토대로 문장을 생성할 수 있다.\n",
        "def generate_sentence(seed=None, debug=False):\n",
        "  if seed is not None:\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "  c = \"<s>\" # 시작 토큰 <s>\n",
        "  sentence = []\n",
        "  while True:\n",
        "    if c not in cpd:\n",
        "      break\n",
        "    # n-gram은 학습 데이터에 없는 단어를 추론할 수 없음(빈도가 0)\n",
        "\n",
        "    w = cpd[c].generate() # 다음에 나올 단어 예측\n",
        "\n",
        "    if w == \"</s>\": # 다음에 나올 단어가 종료 토큰 </s>이면 문장 생성을 종료함\n",
        "      break\n",
        "\n",
        "    word = w.split(\"/\")[0] # 단어\n",
        "    pos = w.split(\"/\")[1] # 품사\n",
        "\n",
        "    # 조사, 어미 등을 제외하고 각 토큰은 띄어쓰기로 구분하여 생성한다.\n",
        "    if c == \"<s>\":\n",
        "      sentence.append(word.title())\n",
        "    elif c in [\"`\", \"\\\"\",\"'\",\"(\"]:\n",
        "      sentence.append(word)\n",
        "    elif word in [\"'\", \".\", \",\", \")\", \":\", \";\", \"?\"]:\n",
        "      sentence.append(word)\n",
        "    elif pos in [\"Josa\", \"Punctuation\", \"Suffix\"]:\n",
        "        sentence.append(word)\n",
        "    elif w in [\"임/Noun\", \"것/Noun\", \"는걸/Noun\", \"릴때/Noun\",\n",
        "                \"되다/Verb\", \"이다/Verb\", \"하다/Verb\", \"이다/Adjective\"]:\n",
        "        sentence.append(word)\n",
        "    else:\n",
        "        sentence.append(\" \" + word)\n",
        "    c = w\n",
        "\n",
        "    if debug:\n",
        "      print(w)\n",
        "\n",
        "  return \"\".join(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgWQJXqRuLMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ed432a-97c7-4f00-dab5-410dd52ebbfe"
      },
      "source": [
        "print(generate_sentence(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "도리까지 본 영화 너무... 뭔가.. 최고네요. 하지만.. 눈물 낫다는건 또 영화에 들지 않는다. 근데 뭐야 어떻게 그렇게 착했던 윤재랑은 에바 그린 드레스 소리 듣는거임\"\"\" 에리 욧의 미모로 합성 한 가수 노래와 흥행 놓친 영화다. 사투리 연기 하나 없는 ‘ 스피드 감 넘치는 스릴 넘치는 연기를 이해 되지 못 하시는 분보다 훨 재밌구만 평점을 망처 놓은 듯하다. 영화 보는이로 하여금 불편함을 느꼇을듯\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RdPvL2AuLy8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "698e0d50-7469-4dc8-a2f2-fc577a359bb1"
      },
      "source": [
        "generate_sentence(2, debug=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "도리/Noun\n",
            "까지/Josa\n",
            "본/Verb\n",
            "영화/Noun\n",
            "너무/Adverb\n",
            ".../Punctuation\n",
            "뭔가/Noun\n",
            "../Punctuation\n",
            "최고/Noun\n",
            "네/Suffix\n",
            "요/Josa\n",
            "./Punctuation\n",
            "하지만/Conjunction\n",
            "../Punctuation\n",
            "눈물/Noun\n",
            "낫다는건/Verb\n",
            "또/Noun\n",
            "영화/Noun\n",
            "에/Josa\n",
            "들지/Verb\n",
            "않는다/Verb\n",
            "./Punctuation\n",
            "근데/Adverb\n",
            "뭐/Noun\n",
            "야/Josa\n",
            "어떻게/Adjective\n",
            "그렇게/Adverb\n",
            "착했던/Adjective\n",
            "윤재/Noun\n",
            "랑은/Josa\n",
            "에바/Noun\n",
            "그린/Noun\n",
            "드레스/Noun\n",
            "소리/Noun\n",
            "듣는거/Verb\n",
            "임/Noun\n",
            "\"\"\"/Punctuation\n",
            "에리/Noun\n",
            "욧의/Noun\n",
            "미모/Noun\n",
            "로/Josa\n",
            "합성/Noun\n",
            "한/Determiner\n",
            "가수/Noun\n",
            "노래/Noun\n",
            "와/Josa\n",
            "흥행/Noun\n",
            "놓친/Verb\n",
            "영화/Noun\n",
            "다/Josa\n",
            "./Punctuation\n",
            "사투리/Noun\n",
            "연기/Noun\n",
            "하나/Noun\n",
            "없는/Adjective\n",
            "‘/Foreign\n",
            "스피드/Noun\n",
            "감/Noun\n",
            "넘치는/Adjective\n",
            "스릴/Noun\n",
            "넘치는/Adjective\n",
            "연기/Noun\n",
            "를/Josa\n",
            "이해/Noun\n",
            "되지/Verb\n",
            "못/VerbPrefix\n",
            "하시는/Verb\n",
            "분/Noun\n",
            "보다/Josa\n",
            "훨/Noun\n",
            "재밌구만/Adjective\n",
            "평점/Noun\n",
            "을/Josa\n",
            "망처/Noun\n",
            "놓은/Verb\n",
            "듯/Noun\n",
            "하다/Verb\n",
            "./Punctuation\n",
            "영화/Noun\n",
            "보는이로/Verb\n",
            "하여금/Adverb\n",
            "불편함을/Adjective\n",
            "느꼇을듯/Noun\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'도리까지 본 영화 너무... 뭔가.. 최고네요. 하지만.. 눈물 낫다는건 또 영화에 들지 않는다. 근데 뭐야 어떻게 그렇게 착했던 윤재랑은 에바 그린 드레스 소리 듣는거임\"\"\" 에리 욧의 미모로 합성 한 가수 노래와 흥행 놓친 영화다. 사투리 연기 하나 없는 ‘ 스피드 감 넘치는 스릴 넘치는 연기를 이해 되지 못 하시는 분보다 훨 재밌구만 평점을 망처 놓은 듯하다. 영화 보는이로 하여금 불편함을 느꼇을듯'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wV8yp-J2U5j"
      },
      "source": [
        "### 5. 결과\n",
        "： 위 예제는 코퍼스 내의 등장 빈도에 기반하여 문장을 생성한다. bigram 언어 모델로 생성한 것이기 때문에 인접한 두 단어는 그나마 자연스럽지만 멀리 떨어진 단어와는 전혀 무관한 모습을 보인다. 또한 생성된 문장의 전체적인 문맥이 부자연스러우며 통사적으로 부적절한 모습도 보인다. 이는 코퍼스 내의 정보만으로 제한된 단어 조합만을 고려하는 N-gram 언어 모델의 한계로 보인다. 위 예제는 단순화를 위해 전처리와 규칙 처리를 최소화하였는데, 데이터셋을 늘리고 한국어 특징에 맞게 전처리를 진행한다면 보다 향상된 결과를 얻을 수 있을 것이다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaSov8sY2w1s"
      },
      "source": [
        "<참고>\n",
        "\n",
        "> https://datascienceschool.net/view-notebook/a0c848e1e2d343d685e6077c35c4203b/\n",
        "\n",
        "\n"
      ]
    }
  ]
}